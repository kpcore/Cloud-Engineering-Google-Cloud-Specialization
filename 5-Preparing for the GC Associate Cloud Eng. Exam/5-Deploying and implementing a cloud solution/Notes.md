## 1. Deploying and implementing compute resources

* Well, we'll be discussing deploying and implementing cloud solutions. This section of the exam guide is a bit longer than the others, but don't fret. Here's part of what we'll be covering. First, let's have a look at some of the things you'll need to know about deploying and implementing Compute Engine resources. Here's a list of Compute Engine deployment tasks that you may encounter as questions in the certification exam. Much of this section, and many of the next few sections, test your hands-on knowledge. Since we can't cover everything here in this course, we'll review material on the first two sets of tasks: launching a Compute Engine instance using Cloud Console and Cloud SDK and creating an autoscaled managed instance group. First, let's review what Compute Engine is and what is certainly used for. Compute Engine lets you create and run virtual machines on Google infrastructure. They're no upfront investments, and you can run thousands of virtual CPUs on a system that is designed to be fast and offer consistent performance. You can create a Compute Engine virtual machine instance by using Google Cloud Platform Console or the gcloud command line tool. Your VM can run Linux and Windows Server images provided by Google or customized versions of these images. You can even import images from many of your physical servers. When you create a VM, you'll be asked to pick a machine type, which determines how much memory and how many virtual CPUs it has. This is highest range from very small to very large indeed. If you can't find a predefined type that meets your needs perfectly, you can make a custom VM. Speaking of processing power, if you have workloads like machine learning and data processing that take advantage of GPUs, many GCP zones have GPUs available for you. Just like physical computers need these sort of VMs, you can choose two kinds of persistent storage: standard or SSD. If your application needs high-performance scratch space, you can attach a local SSD. But be sure to store data, permanent values somewhere else, because local SSD's content won't last pass when the VM terminates. That's why other kinds are called persistent disk. Most people start off with standard persistent disk, and that's a default. There are two kinds of VM instance groups: unmanaged and managed. Unmanaged instance groups are collections of instances that are not necessarily identical and don't share a common instance template. You can use unmanaged instance groups to accommodate your pre-existing configurations for load balancing task. Managed instance groups allow you to operate applications on multiple identical VMs. They offer high availability, scalability using auto-scaling and automated updates. You should always use managed instance groups, unless your applications require you to group instances together that aren't identical. You can use instance templates anytime you want to quickly create VM instances based on a pre-existing configuration. Instance templates define the machine type, boot disk image or container image, labels and other instance properties. If you want to create a group of identical instances, you must use an instance template to create a managed instance group, which also allows you to automatically scale your number of instances. Instance templates are designed to create instances with identical configurations. So it's not possible to update an existing instance template or change an instance template after it's been created. If an instance template goes out of date, or you need to make changes to the configuration, create a new instance template.

## 2. Deploying and implementing Kubernetes resources

* Next, let's have a look at some of the tasks required to deploy and implement Kubernetes Engine resources. In particular, the exam guys calls out the need to understand how to deploy Kubernetes Engine cluster. How to deploy containers using pods and how to configure Kubernetes Engine logging and monitoring. But first, let's do a bit of a review on containers and Kubernetes in general. If you aren't familiar with containers, the next few slides will give you a basic overview. Containers give you the independent scalability of workloads in Platform as a Service, and an abstraction layer of the OS and hardware in Infrastructure as a Service. Containers give you an invisible box around your code and its dependencies, with limited access to its own partition of the file system and hardware. It only requires a few system costs to create and starts as quickly as a process. All you need on each host is an OS kernel that supports containers in a container runtime. In essence, you're virtualizing the OS. It scales like Platform as a Service, but gives you nearly the same flexibility as Infrastructure as a Service. With this abstraction, your code is ultra-portable and you can treat the OS and hardware as a black box. So you can go from development, to staging, to production or from your laptop to the Cloud without changing or building anything. If you want to scale, for example, a web server, you can do so in seconds and deploy dozens or hundreds of them depending on the size of your workload on a single host. That's a simple example of scaling one container running the whole application on a single host. You're more likely to want to build your application using lots of containers each performing their own functions as microservices. If you build applications this way, and connect them with network connections, you can make them modular, deploy easily and scale independently across a group of hosts. The hosts can then scale up and down and start and stop containers as demand for your app changes or as host fail. A tool that helps you do this well is Kubernetes. Kubernetes makes it easy to orchestrate many containers on many hosts, scale them as microservices and deploy roll-outs and rollbacks. Now, let's have a closer look at Kubernetes Engine clusters, and why you might want to deploy a containerized application using pods. At its highest level, Kubernetes is a set of APIs that you can use to deploy containers on a set of nodes called the cluster. The system is divided into a set of master components that run as the control plane and a set of nodes that run containers. In Kubernetes, a node represents a computing instinct like a machine. In Google Cloud, nodes are virtual machines running in Compute Engine. You can describe a set of applications and how they should interact with each other, and Kubernetes figures out how to make that happen. Then you deploy containers on nodes using a rapper on one or more containers called a pod. A pod is the smallest unit in Kubernetes that you create or deploy. A pod represents a running process on your cluster as either a component of your application or an entire app. Generally, you only have one container per pod, but if you have multiple containers with a hard dependency, you can package them into a single pod in shared networking and storage. The pod provides a unique network IP and a set of ports for your containers, and options that govern how container should run. Containers inside a pod can communicate with one another using localhost and ports that remained fixed as they've started and stopped on different nodes. A deployment represents a group of replicas of the same pod and keeps the pods running even when nodes, they run on fail. It could represent a component of an application or an entire application. In this case, it's the engine X web server. To see the running engine X pods, run the command kubectl get pods.

## 3. Deploying App Engine and Cloud Function resources

* Next up, App Engine and Cloud Functions. App Engine and Cloud Functions are GCP services you use when you don't want to maintain a special server yourself. We'll first talk about App Engine. Other two App Engine environments, standard and flexible. Standard is the simpler. It offers a simpler deployment experience in a flexible environment and finer-grained auto-scaling. Like the standard environment, it also offers a free daily usage quota for the use of some services. What's sinctive about the standard environment is that low utilization applications might be able to run at no charge. Google provides App Engine software development kits in several languages so that you can test your application locally before you upload it to the real App Engine service. The SDKs also provides simple commands for doing the deployment. App Engine standard environment provides runtimes for specific versions of Java, Python, PHP, and Go. The runtimes also include libraries that support App Engine APIs. But for many applications, the standard environment runtimes in libraries may be all you need. But if you want to code in another language, the standard environment is not right for you, and you'll want to consider the flexible environment. The standard environment also enforces restrictions on your code by making it run as a so-called sandbox, that's a software construct that's independent of the hardware, operating system, or physical location of the server it runs on. The sandbox is part of why App Engine standard environment can scale a manager application in a very fine-grained way. Like all sandboxes, it imposes some constraints. For example, your application can't write to the local file system, will have to write to a database servers instead if it needs to make data persistent. Also, all the request for your application has a 60-second timeout, and you can't install arbitrary third party software. If these constraints don't work for you, that would also be a reason to choose the flexible environment. In this diagram, we see the App Engine standard environment use. You can develop your application and run a test version of it locally using the App Engine SDK. Then when you're ready, you'll use the SDK to deploy it. Each App Engine application runs in a GCP project and automatically provisioned server instances, scales and load balances them. Your application can make calls to a variety of services using dedicated APIs. For example, a NoSQL datastore to make data persistent, caching of that data using memcache, searching, logging, user login, and the ability to launch actions not triggered by direct user requests like task use in a task scheduler. Let's take a few minutes now and look at Cloud Functions, what they do and how they interact with other services in GCP. Cloud Functions is a lightweight event-based asynchronous compute solution that allows you to create small single-purpose functions that respond to Cloud events without the need to manage a server or a runtime environment. You can use these functions to construct applications from bite-sized business logic. You can also use Cloud Functions to connect and extend Cloud services. You are billed to the nearest 100 milliseconds only while your code is running. Cloud Functions are written in JavaScript, Python or Go, and executed in a managed environment on Google Cloud Platform. Events from Cloud Storage and Cloud Pub/Sub can trigger Cloud Functions asynchronously, or you can use HTTP to call them synchronously.

## 4. Deploying and implementing data solutions

* Let's now have a look at some options for handling your cloud solutions data needs. When you have applications and services, it's almost certain you will also have data. Therefore, knowing how to set up data services for your cloud solution is essential. Knowing how to transfer and load data into your data solution is also essential. However, we'll only cover data solution options in this module. There are labs in the recommended quests that will help you learn more about loading data. Let's quickly review our options for data storage and databases. As noted earlier in this course, data storage and database options can be divided into groups several different ways. One way is by how the data is structured; relational, non-relational, etc. Another way is by usage. With some uses potentially a good fit for more than one option. This table focuses on the technical differentiators of the storage services. Each row is a technical specification, and each column is a service. Let's cover each service from left to right. Consider using Cloud Datastore, if you need to store structured objects, or you'd require support for transactions in SQL like queries. This storage service provides terabytes of capacity with a maximum unit of one megabyte per entity. Consider using Cloud Bigtable If you need to store a large amount of structured objects. Cloud BigTable doesn't support SQL queries, nor does it support multi-row transactions. This service provides petabytes of capacity with a maximum unit of 10 megabyte per cell and 100 megabyte per row. Consider using Cloud Storage if you need to sort immutable blobs larger than 10 megabytes, such as large images or movies. This storage service provides petabytes of capacity with the maximum unit size of five terabytes per object. Consider using Cloud SQL or Cloud Spanner if you need full SQL support for an online transaction processing system. Cloud SQL provides terabytes of capacity, while Cloud Spanner provides petabytes. If Cloud SQL doesn't fit your requirements because you need horizontal scalability, consider using Cloud Spanner. The usual reason to store data in BigQuery is to use its big data analysis and interactive querying capabilities. You wouldn't want to use BigQuery, for example, as the back-end data store for an online application.

## 5. Deploying and implementing networking resources

* Section 3.5 of the study guide consist implementing networking resources for your Cloud solution. Networking in resource options for your Cloud solution include VPCs, subnets, custom network configurations, IP address assignments, firewall ingress and egress rules, VPNs, and load balancing. In this subsection of this module, we'll focus mostly on VPC subnets. First, let's go over some VPC network review topics. Here's something that surprises a lot of people who are new to GCP. The Virtual Private Cloud networks that I defined have global scope. They can have subnets in any GCP region worldwide and subnets can span any of the zones that make up a region. You can also have resources in different zones on the same subnet. This architecture makes it easy for you to define your own network layout with global scope. The way a lot of people get started with GCP is to define their own Virtual Private Cloud inside their first GCP project, or they could simply choose the default VPC and get started with that. Regardless, your VPC networks connect your Google Cloud Platform resources to each other and to the Internet. You can segment your networks, these firewall rules restrict access to instances, and create static routes to port traffic to specific destinations. You can choose to create an automode or a custom mode VPC network. Each new network that you create must have a unique name within the same project. Automode networks create one subnet in each GCP region automatically when you create a network. As new regions become available, used subnets in those regions are automatically added to the automode network. IP ranges for the automatically created subnets come from a pre-determined set of ranges. All other broad networks use the same set of IP ranges. Now, let's have a look at how to implement both of these options in turn, in the following two demos.

## 6. Deploying a solution with Cloud Launcher

* Next, we'll look at how Cloud Launcher now known as Marketplace can help you to plug cloud solutions more easily. In particular, we'll look at the Cloud Launcher and what it offers. Cloud Launcher provides a way to get a new server or software resource up and running quickly. It's a tool for quickly deploying functional software packages on Google Cloud Platform. There's no need to manually configure the software, virtual machine instances, storage, or network settings. Although you can modify many of them before you launch if you'd like. Most software packages in the Cloud Launcher catalog are available at no additional charge beyond the normal usage fees for GCP resources. Some Cloud Launcher images however do charge usage fees, particularly those published by third parties which may use commercially licensed software. But they will all show you estimates of their monthly charges before you launch them. Do be aware that these estimates are just but estimates, and in particular they don't attempt to estimate networking costs. Since those will vary based on how you use the applications. A second note of caution, GCP updates the base images for these software packages to face critical issues and vulnerabilities. But it doesn't update software after it's been deployed. Fortunately, you'll have access to the deployed systems so you can maintain them.

## 7. Deploying an application using Deployment Manager

* The next subsection in the exam study guide is deploying an application using Deployment Manager. Specifically, we'll look at using Deployment Manager templates to provision GCP resources. Deployment Manager is an infrastructure management service that automates the creation and management of your Google Cloud Platform resources. Setting up your environment in GCP can entail many steps, so they have compute network and storage resources and keeping track of their configurations. You could do it all by hand if you want to, taking an imperative approach. But it's more efficient to use a template. That means a specification of what the environment should look like, declarative rather than imperative. GCP provides Deployment Manager to let you do just that. It's an infrastructure management service that automates the creation and management of your Google Cloud Platform resources. To use Deployment Manager, you create a template file using either the YAML Markup Language or Python, that describes what you want the components of your environment to look like. Then you give the template to Deployment Manager, which figures out and does the actions needed to create the environment your template describes. As you need to change your environment, edit your template, and then tell Deployment Manager to update the environment to match change. Here's a tip, you can store in version control, your Deployment Manager templates and Cloud Source Repositories.

## 8. Resource list for Module 4

Compute Engine:  https://cloud.google.com/compute/docs/

Cloud Source Repositories   https://cloud.google.com/source-repositories/docs/

Deployment Manager   https://cloud.google.com/deployment-manager/docs/

Instance Groups:  https://cloud.google.com/compute/docs/instance-groups/

Autoscaling:  https://cloud.google.com/compute/docs/autoscaler/

Instance Templates:  https://cloud.google.com/compute/docs/instance-templates/

Create VMs from instance template:  https://cloud.google.com/compute/docs/instances/create-vm-from-instance-template

Creating groups of managed instances with templates:  https://cloud.google.com/compute/docs/instance-groups/creating-groups-of-managed-instances

Using VPC networks:  https://cloud.google.com/vpc/docs/using-vpc

Deployment manager fundamentals:  https://cloud.google.com/deployment-manager/docs/fundamentals

